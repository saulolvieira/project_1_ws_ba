{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0GX6r4wjXwzB/rWCmLx3T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saulolvieira/project_1_ws_ba/blob/main/ba_data_input_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importando as bibliotecas"
      ],
      "metadata": {
        "id": "uZRZP1TN3L2v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oAiMhvN2iF8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "URL do site indicado: \"https://www.airlinequality.com/airline-reviews/british-airways/page/1/?sortby=post_date%3ADesc&pagesize=100\""
      ],
      "metadata": {
        "id": "zi13MFot54CB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funções"
      ],
      "metadata": {
        "id": "--S4_YrhCIDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para buscar as informações básicas de cada comentário ( nota, comentário e data)\n",
        "def get_basic(url):\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "  comments = soup.find_all('article', attrs={'itemprop': 'review'})\n",
        "\n",
        "  # Buscando informações básicas\n",
        "  data_basic = []\n",
        "  for comment in comments:\n",
        "      row = {}\n",
        "      row['review_id'] = comment.find('div', class_='body')['id']\n",
        "      row['date'] = comment.find('meta')['content']\n",
        "      score_elem = comment.find('span', attrs={'itemprop': 'ratingValue'})\n",
        "      if score_elem:\n",
        "        row['score'] = score_elem.text\n",
        "      else:\n",
        "        row['score'] = 0\n",
        "      row['resume'] = comment.find('h2', class_='text_header').text\n",
        "      row['review'] = comment.find('div', class_='text_content').text\n",
        "      data_basic.append(row)\n",
        "\n",
        "  df_basic = pd.DataFrame(data_basic)\n",
        "  return df_basic"
      ],
      "metadata": {
        "id": "qLzvoajndFlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para buscar todas as informações analíticas de cada comentário de cada página (tabela depois do comentário)\n",
        "\n",
        "def get_analytical(url):\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "  comments = soup.find_all('article', attrs={'itemprop': 'review'})\n",
        "\n",
        "  # Buscando informações básicas\n",
        "\n",
        "\n",
        "  # Encontrando todos os ids\n",
        "\n",
        "  id_ = []\n",
        "  for comment in comments:\n",
        "    row = {}\n",
        "    row['id'] = comment.find('div', class_='body')['id']\n",
        "    id_.append(row)\n",
        "\n",
        "  df_id = pd.DataFrame(id_)\n",
        "\n",
        "\n",
        "  # Encontrar todos os parâmetros de todos os id's do dataframe:\n",
        "\n",
        "  df_results = pd.DataFrame()\n",
        "\n",
        "  for i in df_id['id']:\n",
        "    # Conteúdo da página para cada id\n",
        "    div = soup.find('div', {'class': 'body', 'id': i})\n",
        "    div = BeautifulSoup(str(div), 'html.parser')\n",
        "\n",
        "    # Listas vazias para adicionar os valores de cada coluna\n",
        "    titles = []\n",
        "    values = []\n",
        "    review_id = i\n",
        "\n",
        "    # encontre todas as tags <tr> de cada elemento da tabela\n",
        "    rows = [element.find_all('tr') for element in div]\n",
        "    # desempacote a lista de listas em uma única lista\n",
        "    rows = [tr for sublist in rows for tr in sublist]\n",
        "\n",
        "  # extrair títulos e valores da tabela\n",
        "    for row in rows:\n",
        "        title_element = row.find('td', {'class': 'review-rating-header'})\n",
        "        value_element = row.find('td', {'class': 'review-value'})\n",
        "\n",
        "        if title_element is not None and value_element is not None:\n",
        "            title = title_element.text.strip()\n",
        "            value = value_element.text.strip()\n",
        "            titles.append(title)\n",
        "            values.append(value)\n",
        "\n",
        "        # caso exista a classe review-rating-stars, também é possível coletar as informações\n",
        "        rating_stars = row.find('td', {'class': 'review-rating-stars'})\n",
        "        if rating_stars is not None:\n",
        "            title = title_element.text.strip()\n",
        "            star_spans = rating_stars.find_all('span', {'class': 'star'})\n",
        "            star_fill_spans = [span for span in star_spans if 'fill' in span['class']]\n",
        "            value = str(len(star_fill_spans))\n",
        "            titles.append(title)\n",
        "            values.append(value)\n",
        "\n",
        "    # criar um dicionário com as listas de títulos e valores\n",
        "    data = {'review_id': review_id, 'titles': titles, 'values': values}\n",
        "    df = pd.DataFrame(data)\n",
        "    df_results = pd.concat([df_results, df])\n",
        "\n",
        "  df_analytical = df_results.pivot(index='review_id', columns='titles', values='values')\n",
        "\n",
        "  return df_analytical"
      ],
      "metadata": {
        "id": "VdFgtaN4MDiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iterações"
      ],
      "metadata": {
        "id": "WL3Ls_375gkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Buscando as infomações analíticas de todos os comentários da página\n",
        "# define a quantidade de páginas a serem percorridas\n",
        "num_pages = 36\n",
        "all_results = []\n",
        "# loop para chamar a função para cada página\n",
        "for i in range(1, num_pages+1):\n",
        "    url = f\"https://www.airlinequality.com/airline-reviews/british-airways/page/{i}/?sortby=post_date%3ADesc&pagesize=100\"\n",
        "    scraps = get_analytical(url)\n",
        "    all_results.append(scraps)\n",
        "# concatena todos os dataframes em um único dataframe\n",
        "df = pd.concat(all_results)\n",
        "df = df.reset_index()"
      ],
      "metadata": {
        "id": "c7Nd5g94yeAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Buscando as infomações básicas de todos os comentários da página\n",
        "num_pages = 36\n",
        "all_results2 = []\n",
        "# loop para chamar a função para cada página\n",
        "for i in range(1, num_pages+1):\n",
        "    url2 = f\"https://www.airlinequality.com/airline-reviews/british-airways/page/{i}/?sortby=post_date%3ADesc&pagesize=100\"\n",
        "    scraps2 = get_basic(url2)\n",
        "    all_results2.append(scraps2)\n",
        "# concatena todos os dataframes em um único dataframe\n",
        "df2 = pd.concat(all_results2)\n",
        "df2 = df2.reset_index()"
      ],
      "metadata": {
        "id": "XHb-lLkU8SWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Consolidação final"
      ],
      "metadata": {
        "id": "eHx_I49Z5jSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = pd.merge(df2, df, on = 'review_id', how = 'inner')"
      ],
      "metadata": {
        "id": "Y9ulobG4CAC2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}